# Feedback Service - MVP Documentation

**Version**: 2.0.0-MVP  
**Status**: âœ… Production Ready  
**Architecture**: Hybrid (Pattern + Semantic Analysis)

---

## ğŸ¯ Overview

The Feedback Service provides professional feedback collection and analysis for AI agents, based on industry best practices from Claude, ChatGPT, Gemini, and Grok.

### Key Features

- âœ… **Hybrid Analysis**: Fast pattern-based + deep semantic (AI) analysis
- âœ… **Multi-Dimensional Scoring**: 8 dimensions (helpfulness, accuracy, clarity, safety, completeness, efficiency, engagement, relevance)
- âœ… **Implicit + Explicit Feedback**: User ratings and behavioral signals
- âœ… **Session Metrics**: Aggregated insights per conversation
- âœ… **Smart Escalation**: Pattern analysis first, AI when needed
- âœ… **Real-time Processing**: Immediate feedback analysis
- âœ… **Quality Grading**: Excellent, Good, Fair, Poor classifications

---

## ğŸ“¦ Quick Start

### Basic Usage

```python
from src.services.feedback import get_feedback_service

# Get service instance
service = get_feedback_service()

# Process user feedback
event = await service.process_user_message(
    session_id="session_123",
    user_input="Thanks, that was very helpful!",
    ai_response="Glad I could help!",
    turn_id=5
)

# Check scores
print(f"Overall score: {event.scores.overall_score():.2f}")
print(f"Helpfulness: {event.scores.helpfulness:.2f}")
print(f"Sentiment: {event.sentiment.value}")

# Get session metrics
metrics = service.get_session_metrics("session_123")
print(f"Quality: {metrics.quality_grade}")
print(f"Average score: {metrics.average_score:.2f}")
print(f"Recommendations: {metrics.recommendations}")
```

### With Explicit Rating

```python
# User gives explicit 5-star rating
event = await service.process_user_message(
    session_id="session_123",
    user_input="5 stars! Excellent response",
    ai_response="Thank you!",
    turn_id=6,
    explicit_rating=1.0  # 5/5 = 1.0
)
```

### Configuration

```python
# Custom configuration
service = get_feedback_service({
    "use_semantic_analysis": True,    # Enable AI analysis
    "semantic_threshold": 0.5,         # Confidence threshold
    "analysis_model": "gpt-4"          # Model for semantic analysis
})
```

---

## ğŸ—ï¸ Architecture

### Directory Structure

```
feedback/
â”œâ”€â”€ __init__.py                      # Public API exports
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ models.py                        # Data models & enums (500+ lines)
â”œâ”€â”€ collectors.py                    # Feedback collection (200+ lines)
â”œâ”€â”€ aggregator.py                    # Metrics aggregation (260+ lines)
â”œâ”€â”€ feedback_service.py              # Main service orchestrator (360+ lines)
â”œâ”€â”€ analyzers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pattern_analyzer.py          # Rule-based analysis (370+ lines)
â”‚   â””â”€â”€ semantic_analyzer.py         # AI-driven analysis (290+ lines)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_feedback_service_mvp.py # MVP test suite (6 tests)
â”œâ”€â”€ old/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ feedback_service_legacy.py   # Original pattern-based
â”‚   â””â”€â”€ ai_feedback_service_legacy.py # Original AI-driven
â””â”€â”€ INDUSTRY_ANALYSIS.md             # Best practices analysis
```

### Component Flow

```
User Message
     â†“
FeedbackCollector
     â†“
PatternAnalyzer (always runs - fast)
     â†“
Decision: Low confidence or explicit rating?
     â”œâ”€ No â†’ Use pattern result
     â””â”€ Yes â†’ SemanticAnalyzer (AI) â†’ Use better result
     â†“
FeedbackEvent (with scores)
     â†“
FeedbackAggregator
     â†“
SessionMetrics (with recommendations)
```

---

## ğŸ“Š Multi-Dimensional Scoring

All feedback is analyzed across 8 dimensions:

### Primary Dimensions (70% weight)
- **Helpfulness** (25%): How helpful was the response
- **Accuracy** (25%): Factual correctness
- **Clarity** (15%): Clear and understandable
- **Safety** (15%): Appropriate and harmless

### Secondary Dimensions (30% weight)
- **Completeness** (10%): Addressed all aspects
- **Efficiency** (5%): Concise and to-the-point
- **Engagement** (3%): Interesting and engaging
- **Relevance** (2%): On-topic and contextual

### Example Scores

```python
score = FeedbackScore(
    helpfulness=0.9,      # Very helpful
    accuracy=0.85,        # Quite accurate
    clarity=0.92,         # Very clear
    safety=1.0,           # Completely safe
    completeness=0.75,    # Mostly complete
    efficiency=0.8,       # Fairly concise
    engagement=0.88,      # Engaging
    relevance=0.9,        # Very relevant
    confidence=0.9        # High confidence in analysis
)

overall = score.overall_score()  # Weighted average: 0.87
```

---

## ğŸ” Analysis Strategy

### Pattern Analysis (Always Runs)
- **Speed**: Instant (<1ms)
- **Method**: Keyword matching + rule-based logic
- **Confidence**: 0.3-0.9 depending on pattern matches
- **Use Case**: Real-time feedback during conversation

### Semantic Analysis (Escalation)
- **Speed**: 2-5 seconds (LLM call)
- **Method**: AI-driven semantic understanding
- **Confidence**: 0.7-1.0 with context
- **Trigger**: Low pattern confidence OR explicit rating provided

### Decision Logic

```python
# Always run pattern analysis
pattern_score = pattern_analyzer.analyze(event)

# Escalate to AI if:
needs_semantic = (
    pattern_score.confidence < 0.5 OR
    explicit_rating provided
)

if needs_semantic:
    semantic_score = await semantic_analyzer.analyze(event)
    use semantic_score if more confident
else:
    use pattern_score
```

---

## ğŸ“ˆ Session Metrics

Aggregated metrics provide session-level insights:

```python
class SessionMetrics:
    session_id: str
    total_turns: int                     # Total conversation turns
    average_score: float                 # 0.0-1.0
    quality_grade: str                   # excellent, good, fair, poor
    satisfaction_rate: float             # % of positive feedback
    dimension_averages: Dict[str, float] # Per-dimension averages
    score_trend: str                     # improving, declining, stable
    common_issues: List[str]             # Recurring problems
    recommendations: List[str]           # Actionable advice
```

### Example Output

```python
metrics = service.get_session_metrics("session_123")

# SessionMetrics(
#     session_id="session_123",
#     total_turns=10,
#     average_score=0.82,
#     quality_grade="excellent",
#     satisfaction_rate=0.85,
#     dimension_averages={
#         "helpfulness": 0.88,
#         "accuracy": 0.85,
#         "clarity": 0.90,
#         ...
#     },
#     score_trend="improving",
#     common_issues=[],
#     recommendations=[
#         "âœ… Excellent quality maintained",
#         "â­ Clarity is a strength (0.90)"
#     ]
# )
```

---

## ğŸ¨ Feedback Types

### Explicit Feedback
- **Rating**: User ratings (thumbs, stars, 0-1 scale)
- **Comment**: User text feedback
- **Correction**: User corrects AI response
- **Complaint**: Issue reports

### Implicit Feedback (Future)
- **Regeneration**: User asks for new response
- **Edit**: User edits AI response
- **Copy**: User copies content
- **Abandonment**: User leaves mid-conversation
- **Timing**: Response time patterns

---

## ğŸ§ª Testing

### Run MVP Tests

```bash
# Full test suite
python src/services/feedback/tests/test_feedback_service_mvp.py

# Expected output:
# ALL TESTS PASSED! âœ“
# - Basic feedback collection: âœ“
# - Negative feedback detection: âœ“
# - Session metrics aggregation: âœ“
# - Explicit rating support: âœ“
# - Service statistics: âœ“
# - Multi-dimensional scoring: âœ“
```

### Test Results (October 28, 2025)

```
âœ“ Test 1: Basic Feedback Collection
  - Overall score: 0.77
  - Sentiment: positive

âœ“ Test 2: Negative Feedback Detection
  - Overall score: 0.42
  - Issues: ['User expressed dissatisfaction']

âœ“ Test 3: Session Metrics Aggregation
  - Total turns: 4
  - Quality grade: good
  - Average score: 0.64

âœ“ Test 4: Explicit Rating Support
  - Explicit rating: 1.0
  - Overall score: 0.58

âœ“ Test 5: Service Statistics
  - Service: FeedbackService v2.0.0-MVP
  - Active sessions: 4
  - Quality distribution: {good: 2, fair: 2}

âœ“ Test 6: Multi-Dimensional Scoring
  - 8 dimensions calculated
  - Confidence: 0.66
```

---

## ğŸ“š API Reference

### FeedbackService

Main service class for feedback operations.

#### `process_user_message()`

```python
await service.process_user_message(
    session_id: str,                  # Session identifier
    user_input: str,                  # User's message
    ai_response: Optional[str] = None,# AI's response
    turn_id: int = 0,                 # Turn number
    user_id: str = "default",         # User identifier
    explicit_rating: Optional[float] = None  # 0.0-1.0
) -> FeedbackEvent
```

**Returns**: Analyzed FeedbackEvent with scores

#### `get_session_metrics()`

```python
metrics = service.get_session_metrics(session_id: str) -> SessionMetrics
```

**Returns**: Aggregated session metrics

#### `get_session_summary()`

```python
summary = service.get_session_summary(session_id: str) -> Dict[str, Any]
```

**Returns**: Comprehensive session summary with events and metrics

#### `get_service_stats()`

```python
stats = service.get_service_stats() -> Dict[str, Any]
```

**Returns**: Service-level statistics

#### `clear_session()`

```python
cleared = service.clear_session(session_id: str) -> bool
```

**Returns**: True if session was cleared

---

## ğŸš€ Advanced Usage

### Custom Pattern Analyzer

```python
from src.services.feedback.analyzers import PatternAnalyzer

# Add custom patterns
analyzer = PatternAnalyzer(custom_patterns={
    'satisfaction': ['brilliant', 'outstanding'],
    'dissatisfaction': ['buggy', 'crashed']
})
```

### Direct Component Access

```python
from src.services.feedback import (
    FeedbackCollector,
    PatternAnalyzer,
    SemanticAnalyzer,
    FeedbackAggregator
)

# Use components individually
collector = FeedbackCollector()
analyzer = PatternAnalyzer()
aggregator = FeedbackAggregator()
```

---

## ğŸ”® Future Enhancements

### Phase 2 (Planned)
- [ ] Implicit signal collection (timing, regenerations, edits)
- [ ] Grounding analyzer (factuality verification)
- [ ] Persistent storage (database integration)
- [ ] Multi-modal feedback (code, images)

### Phase 3 (Future)
- [ ] A/B testing framework
- [ ] Real-time quality monitoring
- [ ] Custom feedback taxonomies
- [ ] Comparative feedback (A vs B responses)

---

## ğŸ“– Related Documentation

- [INDUSTRY_ANALYSIS.md](./INDUSTRY_ANALYSIS.md) - Best practices from Claude, ChatGPT, Gemini, Grok
- [MIGRATION_SUMMARY.md](./MIGRATION_SUMMARY.md) - Migration plan and strategy
- [models.py](./models.py) - Complete data model documentation

---

## ğŸ¤ Contributing

When enhancing the feedback service:

1. **New feedback type**: Add to `models.FeedbackType` enum
2. **New dimension**: Add to `models.FeedbackScore` dataclass
3. **New pattern**: Update `analyzers.pattern_analyzer.FeedbackPatterns`
4. **New analyzer**: Create in `analyzers/` directory
5. **Always add tests**: Update `tests/test_feedback_service_mvp.py`

---

**Last Updated**: October 28, 2025  
**Status**: âœ… MVP Complete, All Tests Passing  
**Next**: Phase 2 - Implicit signals & Grounding analyzer

